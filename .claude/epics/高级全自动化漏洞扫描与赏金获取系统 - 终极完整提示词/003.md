---
name: 扫描引擎核心
status: open
created: 2025-09-13T10:46:04Z
updated: 2025-09-13T10:46:04Z
github: [Will be updated when synced to GitHub]
depends_on: [001, 002]
parallel: false
conflicts_with: []
---

# Task 003: 扫描引擎核心

## 概述
构建高性能、可扩展的漏洞扫描引擎，支持多阶段扫描流水线、智能调度、结果聚合和风险评估。集成多种扫描工具，提供统一的扫描接口和结果格式。

## 技术实现要求

### 1. 扫描引擎架构设计

```python
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional, AsyncGenerator
from enum import Enum
import asyncio
from dataclasses import dataclass, field
from datetime import datetime
import uuid
import json

class ScanPhase(Enum):
    DISCOVERY = "discovery"           # 发现阶段
    RECONNAISSANCE = "reconnaissance" # 侦察阶段
    ENUMERATION = "enumeration"       # 枚举阶段
    VULNERABILITY_SCAN = "vulnerability_scan"  # 漏洞扫描
    EXPLOITATION = "exploitation"     # 利用验证（仅PoC）
    POST_ANALYSIS = "post_analysis"   # 后续分析

class ScanStatus(Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"
    PAUSED = "paused"

class ScanSeverity(Enum):
    INFO = "info"
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

@dataclass
class ScanTarget:
    """扫描目标定义"""
    target: str
    target_type: str  # ip, domain, url, network
    context: Dict[str, Any] = field(default_factory=dict)
    constraints: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        self.target_id = str(uuid.uuid4())

@dataclass
class ScanResult:
    """扫描结果"""
    scan_id: str
    target: ScanTarget
    phase: ScanPhase
    tool: str
    timestamp: datetime
    data: Dict[str, Any]
    severity: ScanSeverity = ScanSeverity.INFO
    confidence: float = 1.0
    false_positive_likelihood: float = 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'scan_id': self.scan_id,
            'target': self.target.target,
            'phase': self.phase.value,
            'tool': self.tool,
            'timestamp': self.timestamp.isoformat(),
            'data': self.data,
            'severity': self.severity.value,
            'confidence': self.confidence,
            'false_positive_likelihood': self.false_positive_likelihood
        }

@dataclass
class ScanJob:
    """扫描作业"""
    job_id: str
    target: ScanTarget
    scan_profile: str
    status: ScanStatus = ScanStatus.PENDING
    created_at: datetime = field(default_factory=datetime.now)
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    results: List[ScanResult] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    error_message: Optional[str] = None
```

### 2. 扫描管道（Pipeline）设计

```python
class ScanPipeline:
    """扫描流水线"""
    def __init__(self, name: str, config: Dict[str, Any]):
        self.name = name
        self.config = config
        self.phases: List[ScanPhase] = []
        self.phase_configs: Dict[ScanPhase, Dict[str, Any]] = {}
        self.setup_pipeline()
    
    def setup_pipeline(self):
        """设置扫描流水线"""
        pipeline_config = self.config.get('pipelines', {}).get(self.name, {})
        
        for phase_name, phase_config in pipeline_config.items():
            try:
                phase = ScanPhase(phase_name)
                self.phases.append(phase)
                self.phase_configs[phase] = phase_config
            except ValueError:
                print(f"Unknown scan phase: {phase_name}")
    
    async def execute(self, target: ScanTarget, job: ScanJob) -> AsyncGenerator[ScanResult, None]:
        """执行扫描流水线"""
        for phase in self.phases:
            if job.status == ScanStatus.CANCELLED:
                break
                
            phase_config = self.phase_configs[phase]
            phase_executor = PhaseExecutor(phase, phase_config)
            
            async for result in phase_executor.execute(target, job):
                yield result

class PhaseExecutor:
    """阶段执行器"""
    def __init__(self, phase: ScanPhase, config: Dict[str, Any]):
        self.phase = phase
        self.config = config
        self.tools = config.get('tools', [])
        self.parallel = config.get('parallel', False)
        self.timeout = config.get('timeout', 300)
    
    async def execute(self, target: ScanTarget, job: ScanJob) -> AsyncGenerator[ScanResult, None]:
        """执行扫描阶段"""
        if self.parallel:
            # 并行执行工具
            tasks = []
            for tool_config in self.tools:
                task = self._execute_tool(target, job, tool_config)
                tasks.append(task)
            
            # 等待所有任务完成并收集结果
            for task in asyncio.as_completed(tasks):
                try:
                    results = await task
                    for result in results:
                        yield result
                except Exception as e:
                    print(f"Tool execution failed: {str(e)}")
        else:
            # 顺序执行工具
            for tool_config in self.tools:
                try:
                    results = await self._execute_tool(target, job, tool_config)
                    for result in results:
                        yield result
                except Exception as e:
                    print(f"Tool execution failed: {str(e)}")
    
    async def _execute_tool(self, target: ScanTarget, job: ScanJob, tool_config: Dict[str, Any]) -> List[ScanResult]:
        """执行单个工具"""
        tool_name = tool_config['name']
        tool_args = tool_config.get('args', {})
        
        # 从工具管理器获取工具实例
        from .tool_manager import ToolManager  # 引用Task 002
        tool_manager = ToolManager.instance()
        tool = await tool_manager.get_tool(tool_name)
        
        if not tool:
            raise Exception(f"Tool {tool_name} not available")
        
        # 执行工具
        result = await tool.execute(target.target, **tool_args)
        
        # 转换为标准扫描结果
        scan_results = self._convert_to_scan_results(
            job.job_id, target, result, tool_name
        )
        
        return scan_results
    
    def _convert_to_scan_results(self, scan_id: str, target: ScanTarget, 
                                raw_result: Dict[str, Any], tool_name: str) -> List[ScanResult]:
        """将工具原始结果转换为标准扫描结果"""
        results = []
        
        if tool_name == 'nmap':
            results.extend(self._parse_nmap_results(scan_id, target, raw_result))
        elif tool_name == 'nuclei':
            results.extend(self._parse_nuclei_results(scan_id, target, raw_result))
        else:
            # 通用结果处理
            result = ScanResult(
                scan_id=scan_id,
                target=target,
                phase=self.phase,
                tool=tool_name,
                timestamp=datetime.now(),
                data=raw_result
            )
            results.append(result)
        
        return results
    
    def _parse_nmap_results(self, scan_id: str, target: ScanTarget, 
                           raw_result: Dict[str, Any]) -> List[ScanResult]:
        """解析nmap扫描结果"""
        results = []
        
        # 解析nmap XML输出或文本输出
        nmap_data = raw_result.get('stdout', '')
        
        # 这里需要实现详细的nmap结果解析逻辑
        # 提取端口、服务、操作系统等信息
        
        # 示例：解析开放端口
        import re
        port_pattern = r'(\d+)/tcp\s+open\s+(\w+)'
        ports = re.findall(port_pattern, nmap_data)
        
        for port, service in ports:
            result = ScanResult(
                scan_id=scan_id,
                target=target,
                phase=self.phase,
                tool='nmap',
                timestamp=datetime.now(),
                data={
                    'type': 'open_port',
                    'port': int(port),
                    'service': service,
                    'protocol': 'tcp'
                },
                severity=ScanSeverity.INFO
            )
            results.append(result)
        
        return results
    
    def _parse_nuclei_results(self, scan_id: str, target: ScanTarget,
                             raw_result: Dict[str, Any]) -> List[ScanResult]:
        """解析nuclei扫描结果"""
        results = []
        
        vulnerabilities = raw_result.get('vulnerabilities', [])
        
        for vuln in vulnerabilities:
            severity_map = {
                'info': ScanSeverity.INFO,
                'low': ScanSeverity.LOW,
                'medium': ScanSeverity.MEDIUM,
                'high': ScanSeverity.HIGH,
                'critical': ScanSeverity.CRITICAL
            }
            
            result = ScanResult(
                scan_id=scan_id,
                target=target,
                phase=self.phase,
                tool='nuclei',
                timestamp=datetime.now(),
                data={
                    'template_id': vuln.get('template-id'),
                    'template_name': vuln.get('info', {}).get('name'),
                    'description': vuln.get('info', {}).get('description'),
                    'matched_at': vuln.get('matched-at'),
                    'raw_data': vuln
                },
                severity=severity_map.get(
                    vuln.get('info', {}).get('severity', 'info'),
                    ScanSeverity.INFO
                )
            )
            results.append(result)
        
        return results
```

### 3. 扫描引擎主类

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
from typing import AsyncGenerator
import sqlite3
import threading

class ScanEngine:
    """扫描引擎主类"""
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls, config: Dict[str, Any]):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self, config: Dict[str, Any]):
        if hasattr(self, 'initialized'):
            return
            
        self.config = config
        self.db_path = config.get('database', {}).get('path', 'data/scans.db')
        self.max_concurrent_scans = config.get('system', {}).get('max_concurrent_scans', 5)
        
        # 组件初始化
        self.pipelines: Dict[str, ScanPipeline] = {}
        self.active_jobs: Dict[str, ScanJob] = {}
        self.job_queue: asyncio.Queue = asyncio.Queue()
        self.executor = ThreadPoolExecutor(max_workers=self.max_concurrent_scans)
        
        # 初始化数据库
        self.init_database()
        
        # 加载扫描管道
        self.load_pipelines()
        
        # 启动任务调度器
        self.scheduler_task = None
        
        self.initialized = True
    
    def init_database(self):
        """初始化扫描数据库"""
        conn = sqlite3.connect(self.db_path)
        
        # 创建扫描作业表
        conn.execute('''
            CREATE TABLE IF NOT EXISTS scan_jobs (
                job_id TEXT PRIMARY KEY,
                target TEXT NOT NULL,
                target_type TEXT NOT NULL,
                scan_profile TEXT NOT NULL,
                status TEXT NOT NULL,
                created_at TEXT NOT NULL,
                started_at TEXT,
                completed_at TEXT,
                metadata TEXT,
                error_message TEXT
            )
        ''')
        
        # 创建扫描结果表
        conn.execute('''
            CREATE TABLE IF NOT EXISTS scan_results (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                scan_id TEXT NOT NULL,
                target TEXT NOT NULL,
                phase TEXT NOT NULL,
                tool TEXT NOT NULL,
                timestamp TEXT NOT NULL,
                data TEXT NOT NULL,
                severity TEXT NOT NULL,
                confidence REAL NOT NULL,
                false_positive_likelihood REAL NOT NULL,
                FOREIGN KEY (scan_id) REFERENCES scan_jobs (job_id)
            )
        ''')
        
        # 创建索引
        conn.execute('CREATE INDEX IF NOT EXISTS idx_scan_results_scan_id ON scan_results(scan_id)')
        conn.execute('CREATE INDEX IF NOT EXISTS idx_scan_results_target ON scan_results(target)')
        conn.execute('CREATE INDEX IF NOT EXISTS idx_scan_results_severity ON scan_results(severity)')
        
        conn.commit()
        conn.close()
    
    def load_pipelines(self):
        """加载扫描管道配置"""
        pipeline_configs = self.config.get('pipelines', {})
        
        for pipeline_name in pipeline_configs:
            self.pipelines[pipeline_name] = ScanPipeline(pipeline_name, self.config)
    
    async def start(self):
        """启动扫描引擎"""
        if not self.scheduler_task:
            self.scheduler_task = asyncio.create_task(self._job_scheduler())
    
    async def stop(self):
        """停止扫描引擎"""
        if self.scheduler_task:
            self.scheduler_task.cancel()
            try:
                await self.scheduler_task
            except asyncio.CancelledError:
                pass
        
        # 取消所有活跃的扫描作业
        for job in self.active_jobs.values():
            job.status = ScanStatus.CANCELLED
        
        self.executor.shutdown(wait=True)
    
    async def submit_scan(self, target: str, scan_profile: str = "default", 
                         metadata: Dict[str, Any] = None) -> str:
        """提交扫描任务"""
        # 验证目标授权
        from .security import SecurityController  # 引用Task 001
        security = SecurityController(self.config)
        
        if not security.validate_target(target):
            raise UnauthorizedTargetException(target)
        
        # 创建扫描目标
        scan_target = ScanTarget(
            target=target,
            target_type=self._detect_target_type(target),
            context=metadata or {}
        )
        
        # 创建扫描作业
        job = ScanJob(
            job_id=str(uuid.uuid4()),
            target=scan_target,
            scan_profile=scan_profile,
            metadata=metadata or {}
        )
        
        # 保存到数据库
        self._save_job(job)
        
        # 添加到队列
        await self.job_queue.put(job)
        
        return job.job_id
    
    async def get_scan_status(self, job_id: str) -> Optional[Dict[str, Any]]:
        """获取扫描状态"""
        job = self.active_jobs.get(job_id)
        if not job:
            job = self._load_job(job_id)
        
        if not job:
            return None
        
        return {
            'job_id': job.job_id,
            'target': job.target.target,
            'status': job.status.value,
            'created_at': job.created_at.isoformat(),
            'started_at': job.started_at.isoformat() if job.started_at else None,
            'completed_at': job.completed_at.isoformat() if job.completed_at else None,
            'results_count': len(job.results),
            'error_message': job.error_message
        }
    
    async def get_scan_results(self, job_id: str, 
                              severity_filter: List[str] = None) -> List[Dict[str, Any]]:
        """获取扫描结果"""
        results = self._load_results(job_id, severity_filter)
        return [result.to_dict() for result in results]
    
    async def _job_scheduler(self):
        """作业调度器"""
        while True:
            try:
                # 限制并发扫描数量
                if len(self.active_jobs) >= self.max_concurrent_scans:
                    await asyncio.sleep(1)
                    continue
                
                # 获取下一个作业
                job = await asyncio.wait_for(self.job_queue.get(), timeout=1.0)
                
                # 启动扫描任务
                task = asyncio.create_task(self._execute_scan(job))
                self.active_jobs[job.job_id] = job
                
                # 任务完成后清理
                def cleanup(task):
                    if job.job_id in self.active_jobs:
                        del self.active_jobs[job.job_id]
                
                task.add_done_callback(cleanup)
                
            except asyncio.TimeoutError:
                continue
            except Exception as e:
                print(f"Scheduler error: {str(e)}")
    
    async def _execute_scan(self, job: ScanJob):
        """执行扫描作业"""
        try:
            job.status = ScanStatus.RUNNING
            job.started_at = datetime.now()
            self._update_job(job)
            
            # 获取扫描管道
            pipeline = self.pipelines.get(job.scan_profile)
            if not pipeline:
                raise Exception(f"Unknown scan profile: {job.scan_profile}")
            
            # 执行扫描
            async for result in pipeline.execute(job.target, job):
                job.results.append(result)
                self._save_result(result)
                
                # 检查是否被取消
                if job.status == ScanStatus.CANCELLED:
                    break
            
            if job.status != ScanStatus.CANCELLED:
                job.status = ScanStatus.COMPLETED
                job.completed_at = datetime.now()
            
        except Exception as e:
            job.status = ScanStatus.FAILED
            job.error_message = str(e)
            job.completed_at = datetime.now()
            
        finally:
            self._update_job(job)
    
    def _detect_target_type(self, target: str) -> str:
        """检测目标类型"""
        import ipaddress
        from urllib.parse import urlparse
        
        # URL
        if target.startswith(('http://', 'https://')):
            return 'url'
        
        # IP地址
        try:
            ipaddress.ip_address(target)
            return 'ip'
        except ValueError:
            pass
        
        # 网络段
        if '/' in target:
            try:
                ipaddress.ip_network(target, strict=False)
                return 'network'
            except ValueError:
                pass
        
        # 域名
        return 'domain'
    
    def _save_job(self, job: ScanJob):
        """保存扫描作业"""
        conn = sqlite3.connect(self.db_path)
        conn.execute('''
            INSERT INTO scan_jobs 
            (job_id, target, target_type, scan_profile, status, created_at, metadata)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (
            job.job_id,
            job.target.target,
            job.target.target_type,
            job.scan_profile,
            job.status.value,
            job.created_at.isoformat(),
            json.dumps(job.metadata)
        ))
        conn.commit()
        conn.close()
    
    def _update_job(self, job: ScanJob):
        """更新扫描作业"""
        conn = sqlite3.connect(self.db_path)
        conn.execute('''
            UPDATE scan_jobs 
            SET status=?, started_at=?, completed_at=?, error_message=?
            WHERE job_id=?
        ''', (
            job.status.value,
            job.started_at.isoformat() if job.started_at else None,
            job.completed_at.isoformat() if job.completed_at else None,
            job.error_message,
            job.job_id
        ))
        conn.commit()
        conn.close()
    
    def _save_result(self, result: ScanResult):
        """保存扫描结果"""
        conn = sqlite3.connect(self.db_path)
        conn.execute('''
            INSERT INTO scan_results 
            (scan_id, target, phase, tool, timestamp, data, severity, confidence, false_positive_likelihood)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            result.scan_id,
            result.target.target,
            result.phase.value,
            result.tool,
            result.timestamp.isoformat(),
            json.dumps(result.data),
            result.severity.value,
            result.confidence,
            result.false_positive_likelihood
        ))
        conn.commit()
        conn.close()
```

### 4. 扫描配置文件

```yaml
# 扫描管道配置
pipelines:
  # 快速扫描
  quick:
    discovery:
      tools:
        - name: nmap
          args:
            scan_type: fast
            timeout: 60
      parallel: false
      timeout: 120
    
    vulnerability_scan:
      tools:
        - name: nuclei
          args:
            severity: medium,high,critical
            timeout: 300
      parallel: false
      timeout: 400
  
  # 完整扫描
  comprehensive:
    discovery:
      tools:
        - name: nmap
          args:
            scan_type: full
            timeout: 600
      parallel: false
      timeout: 800
    
    enumeration:
      tools:
        - name: gobuster
          args:
            wordlist: common
            timeout: 300
        - name: subfinder
          args:
            timeout: 180
      parallel: true
      timeout: 400
    
    vulnerability_scan:
      tools:
        - name: nuclei
          args:
            severity: low,medium,high,critical
            timeout: 900
      parallel: false
      timeout: 1000
  
  # Web应用扫描
  webapp:
    discovery:
      tools:
        - name: httpx
          args:
            timeout: 60
      parallel: false
      timeout: 120
    
    enumeration:
      tools:
        - name: gobuster
          args:
            mode: dir
            wordlist: web-common
            timeout: 300
      parallel: false
      timeout: 400
    
    vulnerability_scan:
      tools:
        - name: nuclei
          args:
            tags: web,owasp
            severity: medium,high,critical
            timeout: 600
      parallel: false
      timeout: 800

# 扫描约束和限制
constraints:
  rate_limiting:
    enabled: true
    max_requests_per_second: 10
  
  timeout:
    default: 300
    maximum: 3600
  
  concurrent_scans:
    maximum: 5
    per_target: 1
  
  resource_limits:
    max_memory_mb: 2048
    max_cpu_percent: 80
```

## 实施计划

### 阶段1: 核心架构 (1天)
- 定义扫描数据结构和枚举
- 实现扫描管道框架
- 创建数据库模式

### 阶段2: 结果解析器 (1天)
- 实现各工具结果解析器
- 标准化结果格式
- 添加置信度评估

### 阶段3: 扫描调度 (0.5天)
- 实现作业队列和调度器
- 添加并发控制
- 处理任务生命周期

### 阶段4: API接口 (0.5天)
- 提供扫描提交接口
- 实现状态查询功能
- 添加结果检索API

### 阶段5: 测试和优化 (1天)
- 端到端测试验证
- 性能调优
- 错误处理完善

## 验收标准

1. **扫描流水线**: 多阶段扫描管道正常工作
2. **工具集成**: 所有工具正确集成和结果解析
3. **并发控制**: 支持多任务并发扫描
4. **结果标准化**: 统一的结果格式和存储
5. **状态管理**: 完整的作业状态跟踪

## 预计工期
**3天**

## 风险控制
- 资源管理：严格控制内存和CPU使用
- 超时处理：所有操作都有超时保护
- 错误恢复：扫描失败不影响其他任务